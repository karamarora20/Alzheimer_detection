# -*- coding: utf-8 -*-
"""model1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xM2oBT7hT8jNvSGsP4o825tZffKm5UKc
"""

import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import cv2


# from Models.dataset import load_train_data, load_test_data

def load_train_data(img_size):
    """
    This function loads a dataset of images from a specified path, resizes the images,
    and stores them in a pandas DataFrame along with their respective classes.

    The images are categorized into four classes: 'MildDemented', 'ModerateDemented',
    'NonDemented', and 'VeryMildDemented'. The function iterates through each class,
    opens and resizes the images, and appends them to a dictionary along with their class.

    The dictionary is then converted into a pandas DataFrame and returned.

    Returns:
        df_train (pd.DataFrame): A DataFrame containing the resized images and their classes.
    """

    df_train = []  # create a dictionary to store the dataset
    labels = []  # create a list to store the labels

    """# create dataset from the image folder"""

    classes = ['MildDemented', 'ModerateDemented', 'NonDemented', 'VeryMildDemented']
    train_path = ''

    for i in classes:
        if i == 'MildDemented':
            train_path = '../Dataset/train/MildDemented/mildDem'
        elif i == 'ModerateDemented':
            train_path = '../Dataset/train/ModerateDemented/moderateDem'
        elif i == 'NonDemented':
            train_path = '../Dataset/train/NonDemented/nonDem'
        else:
            train_path = '../Dataset/train/VeryMildDemented/veryMildDem'
        for j in range(100000):
            try:
                img = cv2.imread(f'{train_path}{j}.jpg')
                img_gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                labels.append(i)
                df_train.append(cv2.resize(img_gray, (img_size, img_size)).reshape(img_size * img_size))

            except:
                break

    """# save Dataset into a pandas Dataframe"""
    return np.array(df_train), labels


img_size = 150
df_train, labels = load_train_data(img_size)

print(df_train.shape)
# df_train.head()
df_train = pd.DataFrame(df_train)
df_train['class'] = labels

df_train.head()

X, Y = df_train.iloc[:, :-1].values, df_train.iloc[:, -1]
X = X.reshape(X.shape[0], img_size, img_size, 1)

plt.imshow(X[0])

import random

for i in range(0, 9):
    j = random.randint(0, X.shape[0])
    plt.subplot(330 + 1 + i)
    plt.imshow(X[j].reshape(img_size, img_size), cmap=plt.get_cmap('gray'))
    plt.title(Y[j])

from sklearn.model_selection import train_test_split

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

Y_train
# encode the target variable
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
Y_train = le.fit_transform(Y_train)
Y_test = le.transform(Y_test)

from keras.models import Sequential
from keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization
from keras.optimizers import Adam
from keras.losses import categorical_crossentropy
from keras.utils import to_categorical

y_train = to_categorical(Y_train)
y_test = to_categorical(Y_test)

model = Sequential()

model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_size, img_size, 1),
                 padding='same'))  # f=3,p=1,s=1,c=32 img_size=100
model.add(MaxPooling2D(2, 2))  # f=2,s=2 size=100*100*32

model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))  # f=3,p=1,s=1,c=32 size=100*100*32
# add batch normalisation
model.add(BatchNormalization())
# Add pooling layer
model.add(MaxPooling2D(2, 2))  # f=2,s=2 size=50*50*32
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))  # f=3,p=1,s=1,c=64 size=50*50*64
model.add(MaxPooling2D(2, 2))  # f=2,s=2 size=25*25*64

model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))  # f=3,p=1,s=1,c=64 size=25*25*64
model.add(MaxPooling2D(2, 2))  # f=2,s=2 size=12*12*64

model.add(BatchNormalization())
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))  # f=3,p=1,s=1,c=64 size=12*12*64
# Add pooling layer
model.add(MaxPooling2D(2, 2))  # f=2,s=2 size=6*6*64
model.add(Flatten())
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(4, activation='softmax'))

model.compile(optimizer=Adam(lr=0.001), loss=categorical_crossentropy, metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test))

model.save('model.h5')
